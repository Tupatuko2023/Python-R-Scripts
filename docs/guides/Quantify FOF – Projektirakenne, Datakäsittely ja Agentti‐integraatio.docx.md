# Quantify FOF – Projektirakenne, Datakäsittely ja Agentti‐integraatio

## 1\. Repo-analyysi ja yhteensopivuus

**Moniprojektirakenne:** Analyysirepo *Python-R-Scripts* toimii moniprojektirepona: siinä on useita aliprojekteja (esim. *Fear-of-Falling*). Kunkin projektin aineisto ja koodi on eriytetty, mutta yhteisiä kansioita hyödynnetään tarvittaessa (esim. yhteinen tools/ tai scripts/). Projektikohtaisesti noudatetaan tutkittua käytäntöä, jossa arkaluonteista dataa ei koskaan commitoida julkiseen repoon. Esimerkiksi Fear-of-Falling-projektissa raakadatat säilytetään repo-ulkoisesti (paikallisesti data/external/ \-kansiossa) ja niitä käsitellään vain koodilla – suorat muokkaukset raakadatoihin on estetty *guardrail*\-mekanismeilla[\[1\]](https://agentskillsindex.com/en/skills/Tupatuko2023/Python-R-Scripts#:~:text=Diff,mismatches%2C%20and%20output%20discipline%20risks). Tämä tarkoittaa, että raaka-aineisto on immuuni vahingollisille käsin tehdyille muutoksille ja kaikki data-transformaatiot tehdään toistettavasti skripteillä.

**Data- ja manifest-käytännöt:** Fear-of-Falling-projektissa data sijoitetaan projektin omaan data/\-hakemistoon, jonka alla *external*\-alakansio sisältää alkuperäiset (git-ignoroidut) aineistot. Projektissa ylläpidetään myös *manifestia* eli luetteloa aineistoista ja analyysiajoista. Jokainen aineistopäivitys tai analyysiajo kirjautuu manifestiin omana riviänään automaattisesti, yhdessä odotettujen formaattien tarkistusten kanssa[\[2\]](https://mcpmarket.com/tools/skills/fear-of-falling-qc-summarizer#:~:text=,with%20format%20verification). Tähän on olemassa repositoryssa apukirjastoja (“manifest helpers”), jotka huolehtivat rivien lisäämisestä ja formaatin validoinnista. Manifestin avulla data pysyy organisoituna ja dokumentoituna ajan kuluessa[\[3\]](https://mcpmarket.com/tools/skills/fear-of-falling-qc-summarizer#:~:text=summaries,level%20data%20to%20ensure%20compliance). Samoin laadunvalvontaskriptit (kuten *fof-preflight* ja *fof-qc-summarizer*) varmistavat, että jokainen analyysiajo noudattaa sovittuja käytäntöjä: esimerkiksi vaaditut sarakkeet löytyvät Kxx-datasetistä, ulostuloissa ei ole kiellettyjä tietoja, eikä kukaan ole muokannut raakadataa suoraan (”fails closed on raw data edits”[\[1\]](https://agentskillsindex.com/en/skills/Tupatuko2023/Python-R-Scripts#:~:text=Diff,mismatches%2C%20and%20output%20discipline%20risks)). Tulokset ja lokitiedot ohjataan projektin outputs/\- ja logs/\-hakemistoihin (tai vastaaviin), jotteivät ne sekoitu lähdekoodin kanssa.

**Perittävät käytännöt:** Uuteen *Quantify FOF* \-projektiin kannattaa omaksua samat periaatteet: arkaluonteinen raakadata pidetään repo-ulkopuolella, ja analyysikoodi lukee sitä data/external/\-polun kautta (joka on merkitty *.gitignore*\-tiedostoon). Kaikki dataan tehdyt muutokset on toteutettava skripteillä, jotta prosessi on toistettavissa ja auditoitavissa[\[4\]](https://learning.nceas.ucsb.edu/2023-01-arctic/metadata-best-practices-and-data-publishing.html#:~:text=In%20brief%2C%20some%20of%20the,best%20practices%20to%20follow%20are). Manifestikäytäntöä jatketaan: jokainen aineistonversio, käsittelyajo ja tuotettu artifakti kirjataan ylös (mielellään keskitettyyn CSV-manifestiin) metatietoineen. Lisäksi noudatetaan Fear-of-Falling-projektissa hyväksi todettuja dokumentointikäytäntöjä: projektikohtainen README ja *docs/*\-hakemisto sisältävät mm. menetelmäkuvaukset ja data lineage \-kuvauksen, *reports/*\-kansioon kerätään loppuraportteja tai \-tuloksia, ja *tests/*\-kansioon luodaan automaattiset tarkistukset datan eheydelle ja skeemalle. Kaikki tämä varmistaa, että uusi projekti on yhteensopiva repokokonaisuuden standardien kanssa ja että analyysiprosessi on läpinäkyvä ja luotettava[\[3\]](https://mcpmarket.com/tools/skills/fear-of-falling-qc-summarizer#:~:text=summaries,level%20data%20to%20ensure%20compliance).

## 2\. Uuden projektirepojuuren suunnittelu (Quantify FOF)

Uudelle projektille luodaan selkeä hakemistorakenne repojuren alle. Projektille voidaan antaa kansioksi esimerkiksi **Quantify-FOF-Utilization-Costs/**, joka kuvaa sen sisältöä (FOF-aiheinen palvelunkäytön ja kustannusten kvantifiointi). Rakenne mukailee olemassa olevia projekteja ja yleisiä data-analyysiprojektin käytäntöjä:

Quantify-FOF-Utilization-Costs/  
├── README.md                       \# Projektin tarkoitus, tausta, dataperiaatteet, käyttöohjeet  
├── data/  
│   ├── README.md                   \# Kuvaus data-alikansioista ja tietosuojaperiaatteista  
│   ├── external/                   \# Ulkoiset raakadata-aineistot (EI git-repossa; esim. symbolinen linkki tai paikallinen polku)  
│   ├── VARIABLE\_STANDARDIZATION.csv\# Taulukko muuttujien vakioinnista eri lähteistä  
│   ├── VARIABLE\_STANDARDIZATION.md \# Selittävä dokumentaatio muuttujien standardoinnista  
│   ├── Muuttujasanakirja.md        \# Ihmisl-readable muuttujasanakirja (kuvailee kaikki muuttujat ja koodit)  
│   └── data\_dictionary.csv         \# Kone-read. muuttujasanakirja (sarakeotsikot, tyypit, määritelmät, koodistot)  
├── docs/  
│   ├── methodology.md              \# Menetelmäkuvaus (mm. rekisterilinkitys, analyysimenetelmät)  
│   ├── data\_lineage.md             \# Data lineage: aineistojen alkuperä, versiohistoria, linkitykset  
│   └── decisions.md                \# Päätöslokit: projektin aikana tehdyt avainpäätökset, oletukset ja perustelut  
├── scripts/  
│   ├── preprocess\_data.py          \# (Esim.) Python-skripti CSV/XLSX \-datan esikäsittelyyn analyysivalmiiksi  
│   ├── extract\_pdf\_tables.py       \# (Esim.) Skripti PDF-raporttien taulukoiden ja tekstin irrotukseen  
│   ├── extract\_pptx\_content.py     \# (Esim.) Skripti PPTX-diojen tekstin ja metaosion irrotukseen  
│   └── ...                         \# Mahdolliset muut käsittelyskriptit (R-skriptit voidaan sijoittaa erilliseen R/ \-kansioon)  
├── outputs/  
│   ├── analysis\_ready\_data.csv     \# Analyysivalmis yhdistelmädata (ei sisällä tunnisteita, aggregoitu)  
│   ├── summary\_statistics.csv      \# Koosteita (esim. tunnusluvut FOF- ja ei-FOF-ryhmille)  
│   └── ...                         \# Mahdolliset muut lopputulosaineistot (esim. visualisointidatatauluja)  
├── reports/  
│   ├── FOF\_utilization\_analysis.md \# Raportti analyysista (md/pdf muodossa)  
│   └── Figures/                    \# Kuvioiden tiedostot, jos erillisiä  
├── manifest/  
│   ├── dataset\_manifest.csv        \# Lista projektin datafileistä (nimi, koko, checksum, versio, ym.)  
│   └── run\_log.csv                 \# (Vaihtoehtoisesti) loki jokaisesta ajokerrasta (ajon ID, aika, input-versiot, tuotetut outputit)  
└── tests/  
    ├── test\_schema.py              \# Testit: varmistaa datafilejen skeeman (sarakenimet, tyypit)  
    ├── test\_data\_quality.py        \# Testit: rivimäärät, puuttuvat arvot, perusjakaumat kontrollissa  
    └── test\_reproducibility.py     \# Testit: esimerkiksi varmistaako toistuvuus (samat random seed \-tulokset)

*Kansionimien selitykset:*  
\- **README.md:** projektin etusivu, jossa kerrotaan lyhyesti Aim 2 \-tutkimuskysymys, datan alkuperä (MFFP-kohortti, rekisterilinkitys), eettiset ja tietosuoja-periaatteet (mitä dataa ei ole repossa), sekä ohjeet kuinka analyysikoodi ajetaan.  
\- **data/**: sisältää **vain** metadataa ja esimerkkitiedostoja, ei koskaan koko lupa-aineistoa. Data-kansiossa on ohjeistus (README.md), muuttujastandardoinnin dokumentit ja muuttujasanakirjat. Mahdollisesti voidaan sisällyttää pieni **esimerkkiotos** (keinotekoinen tai anonymisoitu) datasta havainnollistamaan rakenteen, mutta lähtökohtaisesti *external/*\-kansio on vain paikallisessa käytössä. data/external/ on git-ignoroitu – esim. repo voi sisältää tiedoston data/external/PLACEHOLDER.txt, jossa ohjeistetaan käyttäjää sijoittamaan tarvittavat CSV/XLSX-tiedostot tähän polkuun. Tämä järjestely varmistaa, että isoja tai luottamuksellisia raakatiedostoja ei koskaan tallenneta versiohallintaan[\[5\]](https://www.linkedin.com/posts/pablo-arino_so-true-for-data-sciencemachine-learning-activity-7338899099816972288-viQS#:~:text=cluttering%20them,foundation%20for%20scalable%2C%20maintainable%2C%20and).

* **docs/**: tänne kootaan projektiin liittyvä dokumentaatio. *methodology.md*: kuvaa tutkimusasetelman ja analyysimenetelmät (esim. deterministinen rekisterilinkitysproseduuri, tilastolliset menetelmät kustannusten laskemisessa). *data\_lineage.md*: esittää tiedon kulun – mistä lähdedatakohorteista ja rekistereistä tiedot tulevat, miten ne yhdistettiin (ilman henkilötietojen paljastamista) ja mihin versioihin analyysissa viitataan. *decisions.md*: listaa tärkeät päätökset tai muutokset projektin aikana (esim. muuttujien luokittelussa tehdyt päätökset, poissulkemiskriteerit), jotta muut tiimin jäsenet ja tekoälyagentit näkevät taustaoletukset.

* **scripts/**: sisältää varsinaiset analyysikoodit (Python- ja/tai R-skriptit). Tässä erotellaan loogisesti eri vaiheet: esim. datan esikäsittely (muuttujien uudelleennimeäminen, puuttuvien arvojen käsittely), analyysimallien ajaminen, tulosten visualisointi. Yleisperiaate on, että skripteistä voidaan toistaa koko analyysiputki alusta loppuun automatisoidusti. Mikäli analyysissa käytetään Jupyter-notebookeja tai R Markdown \-tiedostoja, nekin voidaan sisällyttää esim. *notebooks/*\-kansioon, mutta suositus on kapseloida uudelleenkäytettävä koodi modulaarisiin *.py* tai *.R* \-tiedostoihin ja pitää notebookit vain kommentoituna ajodokumentaationa.

* **outputs/**: tänne tallennetaan analyysin tuottamat *ei-arkaluonteiset* tulosaineistot. Esimerkiksi aggregoidut tulokset (joista yksittäisiä henkilöitä ei voi tunnistaa) voidaan tallentaa CSV/JSON-muodossa. Myös mahdolliset datasta lasketut tunnusluvut tai visualisointien pohjadatat kuuluvat tähän. Näin saadaan konkreettiset luvut ja kuviodatat versioitua ja tarkistettua.

* **reports/**: sisältää lopulliset raportit, kuvamateriaalit ja yhteenvedot. Esimerkiksi tieteellisen artikkelin luonnos (Word/LaTeX) tai automaattisesti generoitu raportti (R Markdownista knitattu PDF) tallennetaan tänne. Myös mahdolliset esitysmateriaalit (esim. PPTX, jos ne eivät ole luottamuksellisia) tai anonyymit case-esimerkit voidaan säilyttää tässä.

* **manifest/**: tämä kansio keskittyy *metatietoon ja seurantaan*. dataset\_manifest.csv toimii datasettiluettelona: jokaisesta projektiin kuuluvasta tiedostosta (data tai dokumentti) kirjataan ainakin nimi, koko, checksumin (MD5) arvo, ja versio tai päiväys[\[6\]](https://nlsp.nasa.gov/Training%20Sheet%20-%20Data%20Submission%20Guidelines.pdf#:~:text=A%20file%20manifest%20helps%20others,for%20readability). Manifesti helpottaa muiden ymmärtää, mitä tiedostoja datasettiin kuuluu, ja varmistaa että mikään tiedosto ei puutu tai korruptoidu huomaamatta[\[7\]](https://nlsp.nasa.gov/Training%20Sheet%20-%20Data%20Submission%20Guidelines.pdf#:~:text=computing%20languages%20or%20built,with%20files%20will%20go%20undetected)[\[6\]](https://nlsp.nasa.gov/Training%20Sheet%20-%20Data%20Submission%20Guidelines.pdf#:~:text=A%20file%20manifest%20helps%20others,for%20readability). Myös lupa- ja omistajuustiedot voidaan sisällyttää (esim. sarakkeet *Data\_controller* ja *Permit\_ID* kuvaamaan, kuka omistaa datan ja millä luvalla se on saatu käyttöön). Jos analyysiputkea ajetaan toistuvasti päivittyvillä aineistoilla, voidaan ylläpitää myös erillistä run\_log.csv\-tiedostoa, jonne jokainen ajokerta rekisteröidään (sis. ajon aikaleima, käytetyt input-versiot, koodin versio/commit, tuoreet output-tiedostot). Tämä tukee toistettavuutta ja versiokontrollia datan elinkaaren yli[\[8\]](https://nlsp.nasa.gov/Training%20Sheet%20-%20Data%20Submission%20Guidelines.pdf#:~:text=%E2%80%A2%20Specify%20details%20for%20each,%28Continued%20back%20page).

* **tests/**: automaatiotestit varmistavat analyysin luotettavuuden. Minimissään tulisi toteuttaa: skeematestit (tarkistaa, että esim. *analysis\_ready\_data.csv* sisältää juuri ne sarakkeet ja tietotyypit kuin odotetaan – jos rekisterinpitäjä muuttaisi muuttujan nimeä tai tyyppiä, testi hälyttää), laatu- ja loogisuustestit (esim. rivimäärien täsmäävyys eri vaiheissa, tärkeiden muuttujien arvoalueiden plausibiliteetti, FOF-status jakautuu vain sallituille koodeille, summamuuttujat \= osatekijöiden summa, jne.), sekä toistettavuustestit (esim. ajanhetkestä tai suoritusympäristöstä riippumatta analyysikoodi tuottaa saman lopputuloksen, kun siemenluku on vakio). Nämä testit voidaan toteuttaa Pythonissa (pytest) tai R:ssä (testthat) ja ne ajetaan jokaisessa commitissa/CI-pipeline:ssa, jotta mahdolliset virheet datassa tai koodissa huomataan ajoissa.

Yllä kuvattu rakenne on linjassa Fear-of-Falling-projektin kanssa, mutta täsmentää entisestään datahallinnan periaatteita. Rakenne noudattaa myös yleisiä suosituksia data-analyysiprojektin organisoimiseksi: esimerkiksi Cookiecutter Data Science \-malli ehdottaa erottamaan raakadatan (read-only) ja prosessoidun datan eri kansioihin reproducibilityn takaamiseksi[\[9\]](https://www.linkedin.com/posts/pablo-arino_so-true-for-data-sciencemachine-learning-activity-7338899099816972288-viQS#:~:text=So%20true%21%20For%20data%20science%2Fmachine,tool%20for%20handling%20both%20dependency), ja välttämään datan tallentamista GitHubiin (mieluummin viitataan ulkoiseen sijaintiin)[\[5\]](https://www.linkedin.com/posts/pablo-arino_so-true-for-data-sciencemachine-learning-activity-7338899099816972288-viQS#:~:text=cluttering%20them,foundation%20for%20scalable%2C%20maintainable%2C%20and). Tämä suunnitelma toteuttaa juuri nämä periaatteet ylläpitämällä *external*\-kansiota (jota ei versiroida) raakadatalle ja dokumentoimalla kaikki muutokset metadata- ja manifestitiedostoissa.

## 3\. Pitäisikö tehdä data-kansio ja standardointitiedostot?

**Kyllä,** ehdottomasti. Uuden projektin tulee sisältää oma **data/\-kansio**, joka sisältää ainoastaan metadataa, dokumentaatiota ja mahdollisia esimerkkiaineistoja. Tämä noudattaa Fear-of-Falling-projektin mallia: myös siellä on data-kansio, vaikkakin varsinainen raakadata pysyy repo-ulkopuolella. Data-kansio on tärkeä siksi, että se toimii projektin *dokumentoituna totuuden lähteenä* datan rakenteesta ja muuttujista. Data-kansion tiedostot, kuten muuttujien standardointilistat ja sanakirjat, tarjoavat yhteisen viitekehyksen tiimille ja tekoälyagenteille ymmärtää, mitä kukin muuttuja tarkoittaa ja miten eri lähteiden tiedot on yhdistetty. Harvardin datanhallinnan ohjeistuksen mukaan data dictionary (muuttujasanakirja) on kriittinen työkalu toistettavuuden kannalta – sen avulla ulkopuolinenkin ymmärtää datasetin muuttujia ja sisältöä[\[10\]](https://datamanagement.hms.harvard.edu/collect-analyze/documentation-metadata/data-dictionary#:~:text=A%20data%20dictionary%20is%20a,others%20to%20understand%20your%20data). Samoin keskeinen suositus data standardoinnissa on pitää *keskitetty muuttujasanakirja*, jossa määritellään nimikonventiot, tietotyypit, mittayksiköt ja sallitut arvot yhdenmukaisesti kaikille käyttäjille[\[11\]](https://www.intellectyx.com/top-10-best-practices-for-effective-data-standardization-in-2025/#:~:text=Maintain%20a%20Centralised%20Data%20Dictionary).

Projektin *data/*\-kansioon luodaan seuraavat tiedostot ja niille sopivat kentät:

* **README.md (data-hakemistossa):** Selittää, mitä data-kansiossa on. Kuvaa esim. että raakadata sijaitsee *external*\-kansiossa gitin ulkopuolella, sekä ohjeistaa miten analyysiskriptit odottavat datan löytyvän (esim. polkumuuttujan kautta). Voi sisältää myös taulukkomuotoisen listan projektin keskeisistä datasetistä (nimi, kuvaus, rivimäärä, viimeisin päivitys).

* **VARIABLE\_STANDARDIZATION.csv:** Taulukkomuotoinen lista eri lähteistä peräisin olevien muuttujien standardoinnista. Tämä tiedosto on eräänlainen “muuttujien kartta”, joka varmistaa että esimerkiksi rekisteriaineiston muuttuja X ja kyselyaineiston muuttuja Y, jotka molemmat edustavat “kaatumispelon asteikkoa”, yhdistetään ja nimetään analyysissä samalla tavalla. Minimissään sarakkeet voivat olla: *Source (aineistolähde)*, *Original\_Variable\_Name (alkuperäinen nimi lähdedatassa)*, *Standard\_Variable\_Name (vakioitu nimi analyysissa)*, *Description/Notes (lyhyt selite tai koodimuunnos)*. Tarvittaessa mukaan *Unit* (yksikkö, jos eri lähteissä eri mittayksiköt) ja *Coding* (esim. jos arvot luokiteltiin uudelleen: “1/0” \-\> “True/False”). Tiedosto toimii sekä dokumentaationa että käytännön ohjeena: analyysikoodissa voidaan käyttää tätä CSV:tä automatisoimaan muuttujien uudelleennimeämistä ja tyyppimuunnoksia lähdedatasta standardimuotoon.

* **VARIABLE\_STANDARDIZATION.md:** Tämä on tekstimuotoinen selitysdokumentti muuttujien standardoinnista. Siinä voidaan kertoa yleisperiaatteet: *mitkä lähteet datassa on yhdistetty*, *mitkä muuttujat vastaavat toisiaan*, *mitä uusia johdettuja muuttujia on luotu* ja *mitkä koodistomuutokset* on tehty. Esimerkiksi: “MFFP-kyselylomakkeen FOF-mittari (FES-I pistemäärä) yhdistettiin EHR-rekisterin *pelko kaatumisesta* \-muuttujaan; analyysissä käytetään nimeä FOF\_score skaalattuna välille 0–100.” Markdown-muodossa voi myös esittää taulukoita tai listoja keskeisistä muutoksista. Tämä dokumentti palvelee ihmislukijoita (myös tekoälyagentteja, jotka voivat tiivistää siitä vastauksia) ymmärtämään, miten data harmonisoitiin.

* **Muuttujasanakirja.md** (suomeksi kirjoitettu muuttujasanakirja): Tämä on *ihmisluettava* kuvaus kustakin analyysidatassa olevasta muuttujasta. Se on sisällöltään koodikirjan kaltainen: jokaiselle muuttujalle kerrotaan sen *merkitys*, *mittayksikkö*, *mahdolliset arvot/kategoriat* ja *tulkinta*. Esimerkiksi: “FOF\_status: **Kaatumisen pelko \-status.** Binäärimuuttuja, jonka arvo 1 tarkoittaa että henkilöllä on merkittävä kaatumisen pelko, ja 0 tarkoittaa ei merkittävää pelkoa. Mitattu MFFP-kyselylomakkeen kysymyksillä X, Y, Z; positiivinen jos FES-I ≥ 23.” Muuttujasanakirja.md voidaan jäsentää esim. taulukoksi (Markdownin table-syntaksilla: muuttujan nimi, selite, yksikkö, arvot, data-tyyppi). Tämä dokumentti on projektin kannalta olennainen – se on se *yksi totuuden lähde* muuttujien merkityksille, mikä vähentää tulkintaepäselvyyksiä[\[11\]](https://www.intellectyx.com/top-10-best-practices-for-effective-data-standardization-in-2025/#:~:text=Maintain%20a%20Centralised%20Data%20Dictionary). Myös Harvardin ohjeistuksen mukaan hyvä muuttujasanakirja sisältää ainakin muuttujan nimen, ihmisen luettavan selitteen, mittayksiköt, sallitut arvot ja selityksen siitä mitä muuttuja mittaa[\[12\]](https://datamanagement.hms.harvard.edu/collect-analyze/documentation-metadata/data-dictionary#:~:text=%2A%20Variable%20names%20%2A%20Human,Definition%20of%20the%20variable). Juuri nämä tiedot on tarkoitus sisällyttää Muuttujasanakirja.md:hen.

* **data\_dictionary.csv:** Koneellisesti luettava versio muuttujasanakirjasta. Käytännössä tämä CSV sisältää samat tiedot kuin edellä kuvattu .md, mutta rakenteisessa muodossa. Sarakkeita voivat olla: *Variable\_Name*, *Label (lyhyt kuvaus)*, *Data\_Type*, *Allowed\_Values*, *Units*, *Source\_Dataset*, *Definition (pitempi kuvaus)*, *Notes*. Esimerkiksi FOF\_status, Binary indicator of fear of falling, integer (0/1), {0,1}, n/a, MFFP survey, Self-reported fear of falling status based on question X, .... Tämän tiedoston idea on mahdollistaa automaattinen hyödyntäminen: analyysikoodissa tai agenttihaussa voidaan lukea CSV ja vaikkapa tehdä siitä sanakirjahakuja. Lisäksi CSV-muoto varmistaa, että metadata on myöhemmin helposti siirrettävissä muihin järjestelmiin (esim. jos julkaistaan datarepositoryyn, monesti vaaditaan CSV-muotoinen muuttujalista).

Yhteenvetona: **kyllä,** data-kansio ja standardointitiedostot ovat tarpeen. Ne parantavat reproducibilityä ja yhteistyötä, koska kaikki projektissa ymmärtävät datan sisällön yhdenmukaisesti. Muuttujien määrittely ja dokumentointi on oleellisen tärkeää, jotta tekoälyagentit voivat luottaa datan semantiikkaan eivätkä joudu arvailemaan muuttujien tarkoitusta. Ilman kunnollista sanakirjaa riskinä on, että dataa tulkitsee väärin joko ihminen tai tekoäly. Hyvä data dictionary poistaa tämän riskin toimien “yhtenä totuutena” datan määritelmille[\[11\]](https://www.intellectyx.com/top-10-best-practices-for-effective-data-standardization-in-2025/#:~:text=Maintain%20a%20Centralised%20Data%20Dictionary). Mikäli standardointitiedostoja ei luotaisi data-kansioon, pitäisi ne kuitenkin dokumentoida jossain – käytännössä erillinen paikka hankaloittaisi ylläpitoa. Siksi on perusteltua sijoittaa ne suoraan projektiin, jolloin päivitykset kulkevat versionhallinnassa ja kaikki tiimin jäsenet (ja agentit) pääsevät niihin käsiksi.

*(Huom:* *Mikäli jokin organisaation tietohallintostandardi vaatisi keskitettyä metadata-rekisteriä erillään koodireposta, voisi vaihtoehtona olla ylläpitää muuttujasanakirjaa esim. SharePointissa. Tässä tapauksessa repossa olisi linkki tai skripti hakemaan uusin versio. Kuitenkin, sujuvin ratkaisu on sisällyttää sanakirjat repoan, koska ne eivät sisällä henkilötietoja sinänsä, vaan kuvailutietoa.*)

## 4\. paper\_02-aineiston esikäsittely agenttikäyttöön (CSV, XLSX, PDF, PPTX)

Seuraavaksi hahmotellaan **paras mahdollinen esikäsittelyputki** *Quantify FOF* \-projektin aineistoille, jotta tekoälyagentit voivat hyödyntää niitä luotettavasti RAG-tyyppisessä analyysissä. Tavoitteena on tuottaa *agenttiystävällinen* “knowledge package” ilman että luvanvaraista raakadataa tarvitsee tallentaa repoon. Käymme läpi osa-alueittain (A–F) tarvittavat vaiheet ja artefaktit:

**A) Inventaario (aineistoluettelo):** Ensimmäinen vaihe on luoda kattava inventaario kaikista *paper\_02* \-aineiston tiedostoista. Tämä tarkoittaa käytännössä **manifestin luontia**, kuten edellä kuvattiin manifest/-kansiossa. Kirjoitetaan skripti (esim. Pythonin os\- ja hashlib\-kirjastoilla tai R:ssä), joka käy läpi *paper\_02* \-kansion kaikki tiedostot ja kerää niistä seuraavat metatiedot: *tiedostonimi*, *koko (tavuina)*, *MD5-checksum*, *viimeisin muokkauspäiväys*, *tiedostotyyppi/formaatti*, sekä mahdollisesti *omistaja/lähde* ja *luvan tunniste*. Näistä tiedoista muodostetaan taulukko (CSV), joka tallennetaan manifest/dataset\_manifest.csv\-tiedostoon. Checksum-arvojen sisällyttäminen on kriittistä, koska niiden avulla varmistetaan tiedostojen eheys – ilman tarkistesummia voisi jäädä huomaamatta, jos jokin tiedosto korruptoituu tai muuttuu[\[7\]](https://nlsp.nasa.gov/Training%20Sheet%20-%20Data%20Submission%20Guidelines.pdf#:~:text=computing%20languages%20or%20built,with%20files%20will%20go%20undetected). Manifest toimii myös navigointikarttana datasetille: siitä ulkopuolinen (tai agentti) näkee yhdellä silmäyksellä, mitä tiedostoja aineistoon kuuluu ja mihin tarkoitukseen[\[6\]](https://nlsp.nasa.gov/Training%20Sheet%20-%20Data%20Submission%20Guidelines.pdf#:~:text=A%20file%20manifest%20helps%20others,for%20readability). Esimerkiksi manifestin sarakkeet voivat olla: *FileName*, *Description*, *Size(bytes)*, *MD5*, *LastModified*, *Source/Owner*, *AccessPermit*. *Description* kenttään voidaan kirjata lyhyt kuvaus (esim. “FOF-kyselydata, CSV, 1000 riviä” tai “Tulospöytäkirja, PDF, 20 sivua”). Manifesti auttaa myös jäljittämään, onko jokin tiedosto puuttunut siirrossa: kun datatoimittaja lähettää aineiston, voidaan checksumeilla tarkistaa, että kaikki tuli perille[\[13\]](https://nlsp.nasa.gov/Training%20Sheet%20-%20Data%20Submission%20Guidelines.pdf#:~:text=%E2%80%A2%20Include%20MD5%20checksums%20in,with%20files%20will%20go%20undetected). Lisäksi, koska *Quantify FOF* perustuu luvanvaraiseen rekisterilinkitykseen, manifestiin voidaan lisätä yleistasolla lupatiedot (esim. kaikki rekisteridata Permit \#1234:llä) – kuitenkaan henkilötason lupadokumentteja ei tuoda repoon, pelkkä tunniste riittää muistuttamaan käyttöehdoista.

**B) Strukturoitu data (CSV/XLSX) – puhdistus ja harmonisointi:** Kun raakatiedostot on inventoitu, seuraava vaihe on valmistella varsinainen analyysiaineisto. *Paper\_02* aineistot sisältänevät taulukkotietoa CSV- ja Excel-muodoissa (esim. kohorttitiedot, kustannus- ja palvelukäyttötiedot). Paras käytäntö on: **muunna kaikki Excel-tiedostot CSV-muotoon** heti alkuun. Excel-tiedostot ovat hankalia versioida ja niiden sisältö voi olla piilossa monimutkaisissa soluissa; CSV varmistaa avoimuuden ja helpon jatkokäsittelyn[\[14\]](https://nlsp.nasa.gov/Training%20Sheet%20-%20Data%20Submission%20Guidelines.pdf#:~:text=%E2%80%A2%20Tabular%20data%20should%20be,redundant%20data%20entries%20are%20present). Eli jokaiselle XLSX:lle tehdään vastaava .csv (mahdollisesti data/external/\-kansioon lokaalikopioon, mutta repoon ei vieläkään commitoida sisältöä).

Seuraavaksi suoritetaan **skeeman inferenssi ja tietotyyppien yhdenmukaistus**: ladataan CSV-data esimerkiksi Pandasilla (Python) tai datatable/dplyr:llä (R) ja tarkistetaan sarakeotsikot, tietotyypit ja arvoalueet. Kaikki muuttujat pyritään konvertoimaan semanttisesti oikeaan data-tyyppiin: päivämäärämuuttujat (esim. palvelukäynnin päivämäärä) parsetaan date-tyypiksi yhteisessä formaatissa (esim. ISO-8601 YYYY-MM-DD), luokittelumuuttujat voidaan tarvittaessa muuttaa kategorioiksi (factor) ja numeerisiksi tunnistetut arvot (esim. henkilön ikä, kustannussumma) float/integereiksi. Samalla varmistetaan, että puuttuvat arvot on edustettu johdonmukaisesti: käytetään sovittua koodausta, esim. tyhjä kenttä tai NA/N/A merkintä, mutta *vain yksi tapa koko aineistossa*[\[15\]](https://nlsp.nasa.gov/Training%20Sheet%20-%20Data%20Submission%20Guidelines.pdf#:~:text=%E2%80%A2%20Ensure%20all%20values%20are,mm%2Fdd%2Fyyyy%20for%20dates). Jos lähteet käyttävät eri merkintöjä (esim. “.” tai “-” tyhjänä, tai “999” placeholderina), ne muunnetaan konsistentisti.

Seuraava osa on **koodistojen ja luokitusten harmonisointi**: monet rekisterimuuttujat voivat olla koodimuodossa (esim. palvelutyypin koodi, ICD-10 \-diagnoosikoodit). On hyvä liittää mukaan koodien selitykset *jo analyysidataan*, jottei agentin tarvitse arvailla. Tämä voidaan toteuttaa lisäämällä data dictionaryyn koodilistat (esim. taustadokumenttina) tai suoraan datasettiin uusi sarake selitteelle. Ainakin keskeisimmät luokat (esim. ServiceTypeCode \-\> ServiceTypeName) kannattaa avata. **Yksikköjen standardointi** on niin ikään tärkeää: jos kustannuksia on eri vuosilta, päätetään esimerkiksi muuntaa kaikki euroiksi vuoden 2023 arvoon (kerrotaan indekseillä) tai ilmoitetaan selvästi, että eurot ovat jonkun vuoden tasossa. Samoin jos jokin muuttuja on esitetty prosentteina vs. desimaalina eri paikoissa, yhtenäistetään (esim. 0.5 vs 50% \-\> valitaan yksi tapa). Tämä vähentää virhetulkintaa myöhemmin[\[15\]](https://nlsp.nasa.gov/Training%20Sheet%20-%20Data%20Submission%20Guidelines.pdf#:~:text=%E2%80%A2%20Ensure%20all%20values%20are,mm%2Fdd%2Fyyyy%20for%20dates).

**Avainmuuttujien tunnistus:** Erityishuomio kohdistetaan tutkimuskysymyksen kannalta keskeisiin muuttujiin. Näitä ovat: \- **FOF-status** (kaatumispelon ilmaisu). Todennäköisesti binäärinen tai luokallinen muuttuja, joka kertoo onko henkilöllä pelkoa kaatumisesta vai ei (määritelty MFFP-kyselyn perusteella). Tämä muuttuja muodostetaan datasta sovittujen kriteerien mukaan ja tallennetaan esimerkiksi nimellä FOF\_status. On varmistettava, että sen arvot ovat yksiselitteiset (esim. 1 \= FOF, 0 \= ei FOF) ja dokumentoitu sanakirjassa.  
\- **Palvelukäytön määrät**: muuttujat, jotka kuvaavat terveys- ja sosiaalipalvelujen käyttöä (esim. lääkärikäyntien lukumäärä, päivystyskäynnit, kotihoitopäivät tms.). Nämä voivat alun perin olla eri tauluissa; ne yhdistetään yhden henkilön tasolle tarvittaessa. Jos data sisältää tapahtumarivejä, lasketaan summat tai episodikohtaiset aggregaatit analyysitasolle.  
\- **Kustannukset**: kustannusmuuttujat eri palveluista (esim. kokonaiskustannukset euroina per henkilö per vuosi, tai jaoteltuna kustannuslajeittain). Nämäkin yhdistetään henkilötasolle ja mahdollisesti ajanjaksolle (ehkä 1 vuoden seurantakustannus?). Huolehditaan, että kustannusten valuutta ja ajankohta ovat selkeästi ilmoitettu (esim. “EUR 2017 prices”).  
\- **Aikajänne**: muuttujat kuten seuranta-aika, tai indeksiajankohta (milloin FOF-status mitattiin, miltä ajalta kustannukset on koottu). Nämä ovat tärkeitä analyysin tulkinnalle (esim. FOF mitattiin 2016, kustannuksia seurattiin 2017–2019). Nämä tiedot kirjataan dataan selkeinä kenttinä (esim. FOF\_assessment\_date, followup\_period\_years).  
\- **Linkage-avaimet**: itse henkilötunnisteita (kuten *social security number* tms.) **EI** tuoda analyysidataankaan repoon. Mutta analyysin kannalta voi olla tunniste kuten *person\_id*, joka on pseudonymisoitu tai keinotekoinen ID (esim. rekisterinpitäjän generoima random ID per henkilö). Tätä käytetään eri taulujen yhdistämiseen. On varmistettava, että jos analyysikoodia ajetaan, se osaa odottaa oikeaa avainta, mutta ettei missään vaiheessa aito henkilötunnus päädy repoon. Linkitysavaimet pidetään vain paikallisessa datassa. Koodissa voidaan esimerkiksi heti datan latauksen jälkeen pudottaa pois kaikki tunnisteet joita ei tarvita analyysissä, jättäen vain sisäinen id.

Kun data on harmonisoitu ja kaikki tarvittavat muuttujat koottu, tuotetaan **“analysis-ready” taulut**. Tämä voi tarkoittaa esimerkiksi yhtä master-tason taulua, jossa jokainen rivi on yksi henkilö ja sarakkeina kaikki analyysimuuttujat (FOF-status, taustamuuttujat, palvelukäyttöjen määrät, kustannukset). Tällainen *wide format* on hyödyllinen mm. regressioanalyyseissä, joissa jokaisesta henkilöstä halutaan piirteet. Toisaalta voidaan tallentaa myös *tidy data* \-muodossa joitakin osia: esim. palvelukäyttötapahtumat pitkänä tauluna, mikä voi olla hyödyksi jos halutaan agentin vastaavan kysymyksiin yksittäisistä tapahtumista. Tässä on hyvä noudattaa Hadley Wickhamin “tidy data” \-periaatteita: yksi muuttuja per sarake, yksi havainto per rivi[\[16\]](https://learning.nceas.ucsb.edu/2023-01-arctic/metadata-best-practices-and-data-publishing.html#:~:text=,descriptive%20file%20names%20without%20spaces). Mikäli tietoja on useasta ajankohdasta, harkitaan myös tallennetaanko ne laajana (esim. sarakkeet cost\_year1, cost\_year2…) vai pitkänä (rivit: year, cost). Usein pitkän muodon taulu on joustavampi jatkokäsittelyyn, mutta raportointia varten voidaan derivioida laajakin taulu. Ehkä ratkaisu on tallentaa molemmat: yksi laaja yhteenvetotaulu (henkilö per rivi) ja toiset taulut eritellympinä. Kaikista näistä *analysis-ready* \-tauluista tehdään tiedostot outputs/\-kansioon (esim. analysis\_ready\_person\_year.csv tms.), koska ne eivät sisällä enää yksilöiviä tietoja (pseudonymisoitu id korkeintaan). Samalla ne voidaan lisätä manifestiin uudella rivillä (tyyppi “derived data”). Oleellista on, että *kaikki datan käsittelyvaiheet on skriptattu, eikä mitään manuaalista muokkausta tehdä käsin* – näin varmistetaan toistettavuus ja virheettömyys[\[4\]](https://learning.nceas.ucsb.edu/2023-01-arctic/metadata-best-practices-and-data-publishing.html#:~:text=In%20brief%2C%20some%20of%20the,best%20practices%20to%20follow%20are). “Raaka data sisään – puhdas data ulos” \-prosessi koodilla on datanhallinnan kulmakivi.

(Voidaan vielä lisätä, että skeeman yhdenmukaisuuden turvaamiseksi voidaan hyödyntää esimerkiksi *frictionless data package* \-määrittelyä: laaditaan JSON-schema, jossa on odotetut sarakkeet ja niiden tyypit, ja validoidaan lopullinen CSV sitä vasten. Tai Pythonissa Great Expectations \-tarkistuksia. Tämä kuitenkin menee testauspuolelle, josta lisää kohdassa F.)

Yhteenveto vaihe B: meillä on nyt analyysivalmiit taulut, joissa data on puhdasta, *tidy*\-periaatteiden mukaista (esim. “yksi muuttuja per sarake, yksi havainto per rivi”[\[14\]](https://nlsp.nasa.gov/Training%20Sheet%20-%20Data%20Submission%20Guidelines.pdf#:~:text=%E2%80%A2%20Tabular%20data%20should%20be,redundant%20data%20entries%20are%20present)), kaikki arvot on koodattu johdonmukaisesti (puuttuvat arvot, kategoriat, yksiköt standardoitu[\[15\]](https://nlsp.nasa.gov/Training%20Sheet%20-%20Data%20Submission%20Guidelines.pdf#:~:text=%E2%80%A2%20Ensure%20all%20values%20are,mm%2Fdd%2Fyyyy%20for%20dates)), ja henkilö- tai rivitasolla data voidaan vapauttaa analyysin käyttöön (ilman tunnisteita).

**C) PDF-dokumentit – tekstin ja taulukoiden irrotus:** *Paper\_02*\-kansiossa on mainittu olevan PDF-tiedostoja (todennäköisesti paperin käsikirjoitus, artikkeli tai taustaraportteja). Tekoälyagentin kannalta PDF:t ovat arvokkaita, koska niissä on tutkimuksen narratiivi, tulostaulukoita ja mahdollisesti viitteitä. Jotta agentti voi hyödyntää PDF:n sisältöä, meidän täytyy **ekstraktoida** sieltä tieto ja pilkkoa se hallittaviin osiin. Käytämme esim. Python-kirjastoja (PyMuPDF, pdfplumber) tai vastaavia. Vaiheittain:

* **Tekstin nouto:** Luetaan PDF:n tekstit sisältösivu kerrallaan. Poimitaan tekstikappaleet ja otsikot. Monesti PDF-artikkelissa on otsikkotasot (Johdanto, Menetelmät, Tulokset…), jotka haluamme säilyttää hierarkiassa. Jos PDF:ssä on selkeät *heading*\-elementit tai väliotsikot, ne kirjataan ylös (esim. “2. Menetelmät”) ja teksti liitetään siihen. Myös sivunumerot kannattaa tallentaa: jokaiselle tekstikappaleelle voidaan merkitä meta-tietona miltä sivulta se on, jotta agentti voi myöhemmin viitata (esim. “(Lähde: Paper X, s. 14)”).

* **Taulukoiden irrotus:** PDF-tiedostoissa olevat taulukot sisältävät paljon strukturoitua dataa (tulokset, tunnusluvut). Ne eivät välttämättä irtoa pelkällä tekstihaulla, joten käytämme erillistä työkalua (esim. Camelot, Tabula tai Acrobatin export) saadaksemme taulukot koneellisesti luettaviksi. Jokainen taulukko pyritään tallentamaan esim. CSV- tai JSON-muotoon, tai ainakin rakenteelliseksi tekstiksi (rivinnumerot ja sarakkeet eroteltuina). Tärkeää on myös napata taulukon otsikko tai kuvateksti – se kertoo, mitä taulukko esittää.

* **Erotellaan narratiivi ja taulukot erillisiksi paloiksi:** Parhaan hakutuloksen saavuttamiseksi PDF:n sisältö kannattaa *pilkkoa kahtena rinnakkaisena kokonaisuutena*: (1) jatkuva teksti (artikkelin varsinainen teksti ilman isoja taulukoita) ja (2) taulukot (ja mahdolliset kuviotekstit) erikseen. Cohere:n RAG-esimerkissä on havaittu, että PDF:ien sekasisältö hyötyy siitä, että **taulut erotetaan tekstistä ja palastellaan erikseen**[\[17\]](https://docs.cohere.com/page/agentic-rag-mixed-data#:~:text=,that%20similar%20elements%20are%20grouped). Siten agentti voi hakea esimerkiksi numerotietoja taulukoiden joukosta ilman että ne “hukkuvat” tekstihakujen sekaan.

* **Chunking-strategia:** PDF-tekstin pilkkomisessa (chunkkaus) on pidettävä huolta, että paloista tulee sopivan kokoisia ja loogisia. Sisällön katkaisuperiaate voi olla esim. *otsikkokohtainen*: jokainen luku tai alaluku on oma chunk, tai edelleen jaetaan jos teksti on hyvin pitkä. Vaihtoehtoisesti voidaan pilkkoa kappaleittain tai tietyn merkkimäärän mukaan. Tärkeää on, ettei katkaista semanttisesti yhteenkuuluvaa tekstiä pahasti – esimerkiksi yksi kappale kannattaa pitää yhtenä kokonaisuutena, ellei se ylitä esim. 2000 merkkiä. *Sisällöstä riippumattomat* pilkkomisstrategiat (kuten pätkäise 1000 merkkiä kerrallaan) ovat helpoimpia, mutta on syytä varoa ettei esim. lause jää kesken[\[18\]](https://docs.cohere.com/page/chunking-strategies#:~:text=,n%60%20for%20paragraphs). *Sisällöstä riippuvat* strategiat käyttävät tekstirakennetta hyödyksi: esim. PDF:ssä voidaan katkaista juuri kappaleiden lopusta tai otsikoiden kohdalta, jolloin chunkit säilyttävät asiayhteyden[\[19\]](https://docs.cohere.com/page/chunking-strategies#:~:text=%23%20Content). Tässä projektissa hyvä käytäntö voisi olla: *chunkkaa PDF teksti alaluvuittain*, ja jos jokin alaluku (esim. Tulokset) on erittäin pitkä, jaa se kappaleryhmiksi.

* **Metatiedot kuhunkin chunkkiin:** Jokaiseen tekstinpätkään liitetään metadata, mm. \* lähdedokumentin nimi*,* sivunumero(t)*,* lukujen otsikot *ja mahdollisesti* chunkin järjestys*. Esim. chunkin metadata voisi olla: {source: "paper02\_article.pdf", section: "Results", page: 14-15}. Taulukkopalojen meta-tietoihin laitetaan ainakin* taulukon numero/nimi *ja* sivu\*. Nämä metatiedot talletetaan esimerkiksi JSON-muodossa yhdessä chunkin tekstin kanssa.

* **Tallennus ja indeksointi:** Pilkotut tekstipalat voidaan tallentaa tiedostoon (esim. docs/paper02\_article\_chunks.txt tms.) tai suoraan viedä vektorihakuindeksiin. Jotta agentti pystyy käyttämään PDF-tietoa, sen täytyy pystyä hakemaan näitä tekstinpätkiä relevantin kyselyn perusteella. On suositeltavaa luoda *vektoritietokanta* (esim. FAISS, Chroma) näistä palasista, mutta koska kysymys käsittelee repo/dokumenttipainotteista ratkaisua, voimme olettaa että agentti hakee kontekstia suoraan näistä tallennetuista chunk-tiedostoista. Joka tapauksessa järjestellään chunkit esimerkiksi numeroituna listana (chunk1, chunk2, ...) ja säilötään ne vaikkapa Markdown-tiedostoon liitettynä lähdetietoihin.

Yhteenvetona: PDF:stä saadaan ulos *tekstikappaleita* (hierarkian mukaan organisoituina) sekä *taulukkoaineistoa* erillisenä. Erottamalla taulukot omiksi luvuikseen varmistamme, että agentin kyselyihin, joissa haetaan vaikkapa “Palvelukäynnit FOF- vs ei-FOF-ryhmässä”, voidaan vastata suoraan taulukosta jossa todennäköisesti luvut ovat, sen sijaan että agentti yrittäisi poimia numerot proosatekstistä. Tämä strategia – *ryhmitellä samankaltaiset elementit, kuten taulukot yhteen* – on todettu parantavan RAG-suorituskykyä monimuotoisissa dokumenteissa[\[17\]](https://docs.cohere.com/page/agentic-rag-mixed-data#:~:text=,that%20similar%20elements%20are%20grouped).

**D) PPTX (diojen sisältöjen käsittely):** Aineistossa mainitaan myös PPTX-tiedostoja. Ne voivat olla esim. kokouksen esityksiä tai posterin dioja liittyen tutkimukseen. PPTX on rakenteeltaan erilainen haaste: se sisältää dioja, joissa on lyhyitä tekstinpätkiä (bullet pointteja), kuvia, mahdollisesti muistiinpanoja. Tekoälyagentin kannalta diat ovat hyödyllisiä, koska ne tiivistävät tutkimuksen keskeiset tulokset visuaalisesti. Teemme seuraavaa:

* **Diojen tekstin ekstraktio:** Käytetään sopivaa työkalua (Pythonissa python-pptx \-kirjasto, R:ssä officer) lukemaan PPTX-tiedosto. Käydään läpi kukin dia ja poimitaan diojen otsikot, tekstilaatikoiden sisältö sekä mahdolliset **muistiinpanot** (speaker notes), jos niitä on kirjoitettu dioihin. Muistiinpanot voivat sisältää selittäviä lauseita, jotka eivät näy diassa, mutta avaavat sisältöä – ne on hyvä ottaa mukaan, koska ne voivat sisältää laadullista tietoa jota bullet pointit eivät. Samoin, jos dioissa on kuvia kaavioista, yritetään saada kuvan alta mahdollinen tekstimuotoinen kuvateksti.

* **Rakenteinen tallennus:** Jokainen dia tallennetaan esimerkiksi JSON-objektina tai Markdown-muotoisena pätkänä, jossa on *Dia N: Otsikko* ja sitten listana diojen bullet pointit/tekstit. Mukaan liitetään myös diannumero (järjestys) metadataan. Esimerkki tallennus (Markdown):

\*\*Slide 5: Key Findings\*\*    
\- FOF-ryhmän sairaalakäynnit vuodessa: \*\*2,3 (SD 1,1)\*\*, vertailuryhmä: \*\*1,5 (SD 0,9)\*\*.    
\- Ero kustannuksissa: FOF \+1200 € per henkilö/vuosi korkeammat kustannukset.    
\*Notes:\* Tulokset vakioitu iän ja sukupuolen mukaan.

Tässä esimerkissä “Slide 5” on metadataa (dia numero ja otsikko), bulletit on diojen sisältöä ja *Notes* sisältää puhujan muistiinpanoista saadun lauseen.

* **Chunkkaus ja linkitys:** Jokainen dia on yleensä niin tiivis, että yhtä diaa voidaan pitää yhtenä *chunkkina*. Ei yleensä tarvita diakohtaisen tekstin lisäpilkkomista, ellei jokin teksti ole hyvin pitkä. Oleellista on huomioida diojen **järjestys**: joskus edeltävä dia antaa kontekstin seuraavalle. Tätä järjestystä voisi hyödyntää niin, että agentti hakiessaan tietoa dioista tietää myös “naapuridiat”. Käytännössä voimme tallentaa diat listaobjektina, missä niillä on indeksi, ja hakutoiminnallisuus voi halutessa katsoa pari diaa eteen/taakse kontekstin vuoksi.

* **Analyysiväitteiden linkitys dioihin:** Jos PPTX on tutkimuksen oma tulosesitys, on hyödyllistä liittää diat siihen analyysin osaan, johon ne kuuluvat. Esimerkiksi, jos Aim 2 analyysissä on löydös “FOF aiheuttaa X määrän lisäkäyntejä”, ja diassa 5 on graafi siitä, voimme dokumentoida *docs/*\-kansioon, että “Slide 5 corresponds to result Y in the paper, showing increased hospital visits for FOF group”. Tämä voidaan tehdä manuaalisesti lisäämällä *data\_lineage.md*:hen viittaus dioihin. Tai voimme lisätä diojen metadataan tunnisteen kuten related\_result: "IncreasedVisitsAnalysis". Tämä auttaa agenttia, jos se vastaa kysymykseen “Mitä tuloksia FOF:n ja terveyspalvelukäyntien yhteydestä esitettiin?”, niin se osaa hakea sekä paperin tulostekstiä että katsoa löytyisikö dioista jokin havainnollinen luku.

* **PPTX:ien käyttö agentille:** Koska PPTX-diojen teksti on fragmentaarista, ne soveltuvat hyvin suoraviivaiseen *Q\&A*\-hakuun (“Mikä oli FOF-ryhmän kaatumisvammojen kustannus?” saattaa löytyä suoraan yhdeltä dialta numerona). Agentti voi myös pyytää kuvaajia, mutta kuvia se ei suoraan “näe” ilman kuvantunnistusta. Tässä ratkaisussa keskitymme tekstin ja numeerisen sisällön hyödyntämiseen.

Lopputuloksena meillä on PPTX:istä **tekstidokumentti** (tai tietokanta), jossa jokainen dia sisältönsä kanssa on haettavissa. Tämä on ikään kuin tiivistelmä tutkimustuloksista.

**E) Yhteinen metadata ja lineage:** Kun sekä data-aineistot (CSV) että dokumenttiaineistot (PDF, PPTX) on esiprosessoitu, on tärkeää ylläpitää *yhdistävää metadataa* eli ns. *dataset registryä* tai *knowledge basea*, joka kertoo, mitä kaikkea dataa projektiin kuuluu ja miten ne liittyvät toisiinsa. Olemme jo maininneet *manifestin* datasetille (kohta A). Tässä vaiheessa päivitämme manifestiin myös johdetut aineistot ja tehdyt jaot: esim. analysis\_ready\_data.csv lisätään manifestiin (tyypiksi "derived\_dataset", lähteeksi mainitaan alkuperäiset tiedostot ja skripti joka sen tuotti). Voimme laajentaa manifestin sarakkeita lineage-tiedolla: *DerivedFrom* kentässä voisi lukea esim. "survey.csv \+ EHR\_usage.csv (linked by person\_id)". Näin on dokumentoitu datan lineage (polveutuminen).

Lisäksi voimme koostaa korkean tason **“dataset summary card”** kuvaten koko aineistokokonaisuuden: kuka on *data steward*, mitkä ovat *aineistoversiot*, lyhyesti *mitä muuttujia* (viitaten muuttujasanakirjaan), *ajallinen ja alueellinen kattavuus* (esim. "2016 FOF survey in Uusimaa region, linked with 2017–2019 health records in HUS area"). Tämä voisi olla *README.md*:n osa tai erillinen docs/dataset\_card.md. Tällainen dokumentti toimii agentille ja ihmisille nopeana yleiskatsauksena. Se varmistaa, että on olemassa yksi selkeä kuvaus datasetistä – eräänlainen *single source of truth* metatiedoille. Collate-oppaan mukaan data dictionaryn (ja vastaavien metadata-asiakirjojen) ensisijainen tarkoitus on luoda yksiselitteinen totuuden lähde organisaatiolle, joka poistaa sekaannukset datan tulkinnoissa ja helpottaa integraatiota eri lähteiden välillä[\[20\]](https://www.getcollate.io/learning-center/data-dictionary#:~:text=The%20primary%20purpose%20of%20a,where%20inconsistencies%20can%20disrupt%20operations). Juuri tämän teemme: dokumentoimme datan rakenteen niin, ettei epäselvyyttä jää.

Samalla huolehdimme **versiohallinnasta**: kun rekisterinpitäjä toimittaa uuden version aineistosta (esim. päivitetyt kustannustiedot), tallennetaan manifestiin uusi rivi uudelle tiedostolle ja versionumerolle. Jokainen tiedosto saa version tai päivämääräleiman (vaikka data ei repossa ole, voidaan manifestissa pitää versiot). Näin analyysissä on aina jäljitettävissä, millä dataversiolla tulokset on saatu. Manifestiin tai dataset cardiin voidaan merkitä “Data version: 2025-01-15 snapshot”. Samoin, jos analyysikoodia muutetaan, merkitään *run\_log*:iin koodin commit-hash ja data-versio.

Yksi mahdollisuus on hyödyntää *automatisoitua data lineage \-työkalua* (kuten DVC – Data Version Control), joka mahdollistaa datasetin version seurannan ja jopa checksumien automaattisen hallinnan. DVC:ssä voisi luoda *.dvc*\-tiedostot pointtaamaan ulkoiseen dataan. Kuitenkin, ellei haluta lisätyökalun kompleksisuutta, pysytään manuaalisessa manifestissa \+ dokumentaatiossa, mikä riittää projektin kokoluokassa.

**Yhteenveto vaihe E:** luomme keskitetyn “rekisterin” aineistoille. Tämä sisältää manifestin (tiedostolistauksen checksumeineen) ja dokumentaation (dataset card, lineage). Näiden avulla kuka tahansa (tai agentti) voi yhdestä paikasta tarkistaa, mitä dataa on käytetty, mistä se on peräisin ja miten se on muokattu. Tällainen dokumentaatio parantaa compliancea ja läpinäkyvyyttä: se tukee auditointeja ja datan elinkaaren jäljitettävyyttä, mikä on tutkimusdatalle olennaista[\[21\]](https://www.getcollate.io/learning-center/data-dictionary#:~:text=A%20data%20dictionary%20supports%20collaboration,traceability%20of%20the%20data%20lifecycle).

**F) Laadunvarmistus ja toistettavuus:** Viimeisenä, mutta ei vähäisimpänä, rakennetaan automaattinen laadunvarmistus pipeline. Osa tästä on jo mainittu testikansiossa ja edellä:  
\- **Skeematarkistus (schema drift):** Kirjoitetaan testi joka lukee esimerkiksi *analysis\_ready\_data.csv*:n ja vertaa sarakenimiä ja lukumääriä odotettuun. Jos rekisterinpitäjä muuttaisi toimitetussa datassa sarakkeen nimeä tai poistaisi yhden muuttujan, testi havaitsee poikkeaman. Samoin tyyppitarkistus: esim. FOF\_status pitäisi olla binaarinen (0/1); jos yhtäkkiä saa muun arvon, testi hälyttää. Tällaiset drift-havainnot voidaan logittaa ja jopa estää analyysia ajamasta ennen kuin muutos on käsitelty.

* **Arvologiikat:** Määritellään muutamia liiketoimintasääntöjä datalle ja koodataan ne testeiksi. Esim: *FOF\_status \-\> jos arvo \= 1, niin FES-I pistemäärän pitäisi olla ≥ tietty threshold (jos data sisältää pistemäärän)*; *kustannus*\-muuttujien ei pitäisi olla negatiivisia; *palvelukäyntien lukumäärä* ei voi olla desimaali tms. Myös aggregaattitason tarkistus: laske analysis\_ready\_data.csv:stä montako henkilöä FOF\_status=1 ja \=0, ja varmista että luvut täsmäävät alkuperäisen lähderaportin lukuihin (jos saatavilla). Nämä varmistukset estävät hiljaista virhettä – jos jokin yhdistely meni pieleen ja vaikka 100 henkilöä putosi pois, testi kiinnittää huomiota poikkeavaan rivimäärään.

* **Checksumien valvonta (“no raw edits”):** Otetaan talteen alkuperäisten raakatiedostojen MD5-tarkisteet (manifestissa on ne). Voidaan tehdä testi, joka lataa paikalliset data/external\-tiedostot (jos löytyy) ja laskee niiden MD5-hashit ja vertaa manifestiin. Jos erotusta löytyy, se tarkoittaa että raakadatalle on tapahtunut jotain (tiedosto vaihtunut tms.). Tällöin testi epäonnistuu. Näin varmistetaan, että kukaan ei vahingossa ole käsin käynyt muokkaamassa *analysis\_ready\_data.csv*:tä tai mitään muutakaan tiedostoa repoon – jokaisen muutoksen pitää tulla koodin kautta. Tämä on juuri se periaate, josta pidettiin huolta jo Fear-of-Falling-projektissa (“fails closed on raw data edits” \- check)[\[1\]](https://agentskillsindex.com/en/skills/Tupatuko2023/Python-R-Scripts#:~:text=Diff,mismatches%2C%20and%20output%20discipline%20risks).

* **Toistettavuus (reproducibility):** Jotta analyysitulokset ovat toistettavia, on olennaista kontrolloida analyysiprosessin muuttujia. Käytännössä:

* **Ympäristön määrittely:** lukitaan riippuvuudet. Projektissa voidaan käyttää esim. *requirements.txt* (Pythonille) tai *renv.lock* (R:lle) määrittelemään tarkat pakettiversiot. Näin varmistetaan, että vuoden päästäkin ajettaessa samat versiot tuottavat samat tulokset. Testataan, että ympäristön saa pystytettyä ohjeiden mukaan.

* **Pseudo-satunnaisuus:** jos analyysissa on esimerkiksi bootstrap-simulaatioita tai train/test-jakoa, asetetaan random seed vakioarvoon koodissa. Testi voi vaikka ajaa tietyn osan analyysista kahteen kertaan ja verrata, että tulos on identtinen (esim. regressiomallin parametrit).

* **Konfiguraation hallinta:** mikäli analyysikoodissa on parametreja (esim. include/exclude tietyt alaryhmät), ne kerätään yhteen config-tiedostoon (yaml/json). Tällöin jokainen analyysiajo käyttää eksplisiittistä konfiguraatiotiedostoa – tämä tiedosto voidaan tallentaa run\_logiin. Jos tuloksia verrataan, nähdään heti mikä config on ollut käytössä.

* **Lopputulosten validointi:** voidaan rakentaa pieni “sanity check” myös outputeille. Esimerkiksi jos aiemmassa tutkimuksessa (Aim 1 tms.) on raportoitu jokin luku, varmistetaan että uusien tulosten suuruusluokka on järkevä. Tai jos analyysissä lasketaan “FOF:in attribuoitavissa oleva kustannus”, ja sen arvio on 1200 €/hlö, voitaisiin testata ettei se poikkea yli 10-kertaisesti aiemmasta arviosta (jos odotusarvoja on). Tällaiset checkit ovat toki projekti-spesifisiä, mutta agentin kannalta ne varmistavat, ettei se joudu tulkitsemaan aivan virheellisiä tuloksia.

Kaikki edellä mainitut QA-toimet pyritään automatisoimaan CI/CD-putkeen (esim. GitHub Actions). Näin jokainen päivitys koodiin tai dataan laukaisee testit. Jos jokin epäilyttävä muutos on tapahtunut (schema drift, quality issue), korjataan se ennen kuin agentti tai tutkija käyttää aineistoja. Tämä *fail fast* \-periaate on hyvin linjassa Fear-of-Falling-projektin käytäntöjen kanssa, jossa laatuongelmissa mieluummin kaadetaan prosessi kuin jatketaan hiljaa eteenpäin riskillä[\[2\]](https://mcpmarket.com/tools/skills/fear-of-falling-qc-summarizer#:~:text=,with%20format%20verification).

QA-vaiheen viimeinen osa on dokumentoida toistettavuus: kirjoitetaan vaikka *docs/reproducibility.md*, jonne listataan ympäristö (OS, R/Python-versiot), käytetyt siemenluvut, sekä miten analyysin voi toistaa askel askeleelta. Tämä yhdessä manifestien ja testien kanssa varmistaa, että projekti noudattaa avoimen tieteen reproducibility-ihannetta. Kuten yksi data science \-ohje toteaa: *“Immutable Raw Data… This ensures reproducibility.”* – kun raakadata pidetään muuttumattomana ja käsittely on koodissa, saadaan samat tulokset jatkossakin[\[9\]](https://www.linkedin.com/posts/pablo-arino_so-true-for-data-sciencemachine-learning-activity-7338899099816972288-viQS#:~:text=So%20true%21%20For%20data%20science%2Fmachine,tool%20for%20handling%20both%20dependency).

## 5\. Miten aineisto kahlataan niin, että agenttianalyysi onnistuu

Lopuksi hahmotellaan **agenttiystävällinen työnkulku**, jonka avulla tekoälyagentti (esim. ChatGPT tai vastaava RAG-järjestelmä) löytää oikeat tiedot aineistosta ja vastaa luotettavasti niitä hyödyntäen. Idea on, että agentti ei saa suoraa pääsyä kaikkeen, vaan ohjataan sitä hyödyntämään valmisteltua *knowledge packagea* (eli edellä esikäsiteltyjä dokumentteja, sanakirjoja ja koosteita). Tärkeitä elementtejä:

* **Hakemistorakenne ja indeksointisäännöt:** Agentille määritellään, mistä hakemistoista se saa etsiä tietoa. Suojaussyistä rajataan pois kaikki arkaluonteinen: data/external/ ei sisällä mitään agentille (se on tyhjä repossa), joten agentti keskittyy *docs/*\-, *outputs/*\-, *reports/*\- ja *data/*\-kansion julkisiin tiedostoihin. Käytännössä voimme luoda agentille indeksoidun kokoelman kaikista Markdown/CSV/JSON \-tiedostoista näissä kansioissa. Esimerkiksi:

* *docs/*: sisältää metodologia- ja lineage-dokumentit \-\> näistä agentti löytää kontekstia tutkimusasetelmaan (esim. kohdepopulaation kuvaus, linkitystapa).

* *data/*: sisältää muuttujasanakirjat ja standardointilistat \-\> näistä agentti voi tarkistaa muuttujien merkitykset tai katsoa, mistä lähteistä mikäkin muuttuja tuli. Tämä on kullanarvoista esim. kysymyksessä “Miten FOF-status on määritelty?” – agentti voi suoraan poimia määritelmän Muuttujasanakirja.md:stä luotettavasti.

* *outputs/*: sisältää analyysin tulostaulut (aggregoidut). Jos käyttäjä kysyy lukuja, kuten “Paljonko enemmän kustannuksia FOF-ryhmälle kertyy?”, agentti voi etsiä outputs-kansiosta esimerkiksi summary\_statistics.csv:stä relevantin luvun. Koska outputs-data on aggregoitua (ei tunnistetietoja), se on sallittua agentin käyttöön.

* *reports/*: sisältää esim. artikkeliluonnoksen tekstiä tai esitysdioja. Näistä agentti löytää lauserakenteita, joilla voi vastata selittävämmin.

Indeksointisäännöissä voidaan painottaa tiettyjä tiedostoja: esim. Muuttujasanakirja.md ja analysis\_report.md ovat “ensisijaisia” lähteitä tietyntyyppisille kysymyksille. Tiettyä hienosäätöä voi tehdä antamalla agentille ohjeita: *“Jos kysymys koskee muuttujien definioita, katso data/Muuttujasanakirja.md”*. Yleisesti RAG-pipeline toteutetaan niin, että agentti suorittaa ensin vektorihakua sisällöistä ja saa top-n relevanttia chunkia kontekstiksi.

* **Tiivistelmät ja ground truth \-artefaktit:** Jotta agentti suoriutuu tehokkaasti, sille tarjotaan valmiiksi pureskeltua tietoa keskeisistä asioista:

* *Dataset card & analyysikooste:* Yksi tai kaksi kappaletta, jotka tiivistävät koko tutkimuksen aineiston ja päätulokset. Agentti voi käyttää tätä “tl;dr” vastaamaan yleisiin kysymyksiin. (Esim. *“Tutkimuksessa havaittiin, että kaatumisen pelosta kärsivien terveyspalvelujen vuotuiset kustannukset ovat keskimäärin 1200€ korkeammat verrattuna muihin, ja heillä oli 50% enemmän päivystyskäyntejä[\[22\]](https://www.itconvergence.com/blog/how-to-overcome-ai-hallucinations-using-retrieval-augmented-generation/#:~:text=Retrieval,in%20reliable%2C%20external%20data%20sources).”* – tämän tyylinen lause voisi ollakin dataset cardissa valmiina.)

* *Table profiles:* Jokaisesta keskeisestä taulusta (esim. analysis\_ready\_data) voidaan tehdä automaattinen profilointi: muuttujien min/maksimi, keskiarvot, jakaumat. Näitä voi säilöä vaikka JSON-tiedostoina tai raportoida docs/:iin. Tekoäly voi näiden perusteella vastata tarkentaviin kysymyksiin, kuten “Mikä oli FOF-ryhmän ikäjakauma?” – agentti voisi etsiä dataset-profiilista maininnan ikäkeski-iästä.

* *Variable cards:* Erityisen tärkeistä muuttujista (FOF\_status, kustannukset, palvelukäynnit) voidaan luoda yksittäiset markdown-tiedot (“cards”), joissa on syvempi selitys: miten mitattiin, mitä arvoja sai, perusjakauma, mahdolliset confounderit. Tämä on vähän kuin laajempi muuttujasanakirjan artikkeli. Agentti voi hyödyntää näitä tuottaakseen perusteltuja vastauksia (“FOF\_status on määritelty itse raportoitu kaatumisen pelko; tutkimuksessa X% kohortista (N \= Y) raportoi pelkoa[\[12\]](https://datamanagement.hms.harvard.edu/collect-analyze/documentation-metadata/data-dictionary#:~:text=%2A%20Variable%20names%20%2A%20Human,Definition%20of%20the%20variable).”).

* *Kysymys-vastaus pankki:* Vaikka ei kysytty, mainittakoon että voidaan valmistella valmiita Q\&A-pareja (FAQ-tyyppisesti) projektin sisällöstä. Tämäkin auttaa agenteja, mutta se menee jo sisällöntuotannon puolelle.

* **Hallusinaatioiden estäminen:** Retrieval-Augmented Generation \-lähestymistapa itsessään vähentää hallusinaatiota huomattavasti, koska malli nojaa ulkoiseen luotettavaan dataan[\[22\]](https://www.itconvergence.com/blog/how-to-overcome-ai-hallucinations-using-retrieval-augmented-generation/#:~:text=Retrieval,in%20reliable%2C%20external%20data%20sources). Mutta varmistetaan tämä vielä muutamin keinoin:

* *Pakolliset lähdeviitteet:* Konfiguroidaan agentin “vastauksenmuodostus” niin, että sen **täytyy** antaa jokaiselle faktaväitteelle lähde (kuten tämänkin vastaustekstin viitteet). Sovimme, että ilman lähdettä agentti ei esitä uutta tietoa. Tämä ohjeistus voidaan kovakoodata prompttiin tai agentin ohjauslogiikkaan.

* *Lähdelinkit:* Tarjotaan agentille valmiiksi muodostetut cite-nuotteet (kuten [\[6\]](https://nlsp.nasa.gov/Training%20Sheet%20-%20Data%20Submission%20Guidelines.pdf#:~:text=A%20file%20manifest%20helps%20others,for%20readability)), joita se voi liittää. Lisäksi annetaan mahdollisuus agentille kertoa käyttäjälle, mistä tieto on peräisin (esim. “(lähde: dataset\_manifest.csv)”), jotta käyttäjäkin voi arvioida luotettavuutta.

* *Kynnysarvo ja varovaisuus:* Määritetään, että agentti käyttää vain riittävän korkean *relevanssiskoren* omaavia hakutuloksia. Jos kysymykseen ei löydy mitään kontekstia meidän tietopaketista (vektorien etäisyys ei ylitä tiettyä kynnystä), agentti mieluummin **kysyy tarkentavia lisätietoja tai ilmoittaa, ettei tietoa löydy**, kuin arvaa. Tämä estää ns. harhailun aiheen ulkopuolelle.

* *Checklista vastauksen tarkistamiseen:* Agentille voidaan antaa “itsearviointi”-vaihe: kun se on muodostanut vastauksen, se käy läpi listan: “1) Sisältääkö vastaus vain tietoja, joille löytyy viite kontekstista? 2\) Onko jokainen numeerinen arvo tarkistettu lähdedokumentista? 3\) Onko vastaus käyttäjän kysymykseen eikä sivuuta aihetta?” Jos jokin näistä ei täyty, agentti muokkaa vastausta. (Tällainen itse-iterointi on mahdollista agentti-frameworkeissä).

* *Ihmisen varmistus kriittisissä kohdissa:* Erittäin tärkeissä analyyseissä voidaan pitää *ihmissilmukka* niin, että agentin tuottamat tulokset tarkistetaan ainakin aluksi tutkijan toimesta, ennen kuin ne menevät eteenpäin. Kuitenkin automaattisessa analyysirepossa tämä ehkä tapahtuu “pull request review” \-muodossa: agentti tekee analyysiraportin luonnoksen ja ihminen hyväksyy/muokkaa sen.

* **Mitä agentille ei koskaan anneta:** Tärkeää on rajata pois data, joka sisältää henkilötason luottamuksellisia tietoja tai lupaehtoja rikkovaa sisältöä. Käytännössä:

* Yksilötason rekisteririvit **eivät** koskaan päädy agentin nähtäväksi. Agentti ei saa tietää esim. yksittäisen henkilön kustannuksia tai yhdistellä pseudotunnisteita. Nämä pysyvät analyysin “mustassa laatikossa”. Agentille annetaan vain aggregaatiot tai anonymisoidut koosteet, jotka eivät riko yksityisyyttä.

* *Linkage keyt* (esim. henkilön ID) poistetaan aineistoista ennen agentin vaihetta. Agentti ei saa syötettä, josta se edes teoreettisesti voisi tunnistaa jonkun (tässä tapauksessa data on tosin pseudonymisoitu, mutta varman päälle).

* Raakadatan yksityiskohtaiset tiedot (esim. yksittäisen rekisteritapahtuman kuvaus) – näitä agentti ei tarvitse, ellei tutkimuskysymys sitä vaadi, ja ne voisivat johtaa harhaan.

* Lupadokumentit tai eettiset asiakirjat, joissa on osapuolten nimiä, tms. – näitäkään ei syötetä agentille. Ne voidaan pitää irrallaan. Agentille voi välittää niiden sisällön tiivistettynä (“Data used under permit XYZ, only group-level results can be published”), mutta ei itse asiakirjoja.

* Koodi tai tekniset lokit, jotka eivät ole käyttäjän kysymysten kannalta olennaisia. Agentin fokuksen pitämiseksi relevantissa tiedossa emme sisällytä indeksoitavaksi esim. preprocess\_data.py koodia (ellei käyttäjä nimenomaan kysy koodista). Näin vältetään turha “harhapolku” jossa agentti alkaa selittää koodia datan sijaan.

Yllä mainittu agentin toimintatapa on käytännössä RAG-järjestelmän suunnittelua. Tärkeää on, että **aineisto on valmisteltu niin hyvin, että agentin ei tarvitse arvata mitään – se löytää tarvittavat palaset suoraan**. Kun agentti pohjaa vastauksensa suoraan luotettaviin, projektin sisäisiin lähteisiin, hallucinaatioiden riski pienenee dramaattisesti (ulkoiset harhat vähenevät)[\[22\]](https://www.itconvergence.com/blog/how-to-overcome-ai-hallucinations-using-retrieval-augmented-generation/#:~:text=Retrieval,in%20reliable%2C%20external%20data%20sources). Erityisesti se, että annoimme agentille strukturoitua tietoa (taulukoita, sanakirjoja), auttaa sitä pysymään totuudessa – rakenteinen tieto on yksiselitteistä ja vähentää ambiguiteettia[\[23\]](https://www.itconvergence.com/blog/how-to-overcome-ai-hallucinations-using-retrieval-augmented-generation/#:~:text=Use%20Structured%20Data%20for%20Retrieval).

Kaiken huipuksi, jos haluamme lisää varmuutta, voimme toteuttaa vielä **hallusinaatiotarkistimen**: AWS:n esimerkissä mainitaan, että voidaan luoda järjestelmä, joka yrittää tunnistaa hallusinaation agentin vastauksesta (esim. antamalla agentin vastauksen takaisin toiseen malliin joka vertaa sitä lähdeteksteihin)[\[24\]](https://www.itconvergence.com/blog/how-to-overcome-ai-hallucinations-using-retrieval-augmented-generation/#:~:text=...%20www.itconvergence.com%20%20Retrieval,in%20reliable%2C%20external%20data%20sources). Tämä on tosin edistynyt aihe, mutta mainittakoon, että organisaation kannalta kriittisissä sovelluksissa voidaan implementoida “verification agent” tai *LLM-as-a-judge* \-menetelmä, joka varmistaa jokaisen vastauksen paikkansapitävyyden[\[25\]](https://docs.cohere.com/page/agentic-rag-mixed-data#:~:text=More%20broadly%2C%20the%20implementation%20of,the%20model%20access%20to%20tools).

## 6\. Toimitusmuoto (Quick start ensimmäiset 60 min)

Lopuksi koostetaan konkreettinen *askel-askeleelta* etenemissuunnitelma, jotta projekti saadaan käyntiin ja ensimmäisen tunnin aikana edistymään:

1. **Repo-rakenteen luonti:** Luo uusi kansio Quantify-FOF-Utilization-Costs monirepoon. Alusta sen sisälle yllä kuvatut keskeiset kansiot: data/, docs/, scripts/, outputs/, reports/, manifest/, tests/. Voit käyttää edellä esitettyä puumallia apuna. Lisää tarvittavat .gitignore\-merkinnät (esim. data/external/\* ignooraus). Commitoi tämä alustava rakenne versiokontrolliin (“Initialize Quantify-FOF project structure”).

2. **Perusdokumentit alkuun:** Avaa projektille README.md ja kirjoita siihen lyhyt kuvaus Aim 2:sta suomeksi ja englanniksi. Kirjaa ylös datalähteet (”MFFP-kysely, EHR-rekisterit...”), sekä tietosuojahuomio (“Henkilötason data pidetään reposta erillään, analyysit tehdään pseudonymisoidulla datalla.”). Lisää README:hen myös ohje, miten projekti ajetaan (esim. “make run\_all tai ohje skriptien ajamisjärjestykseen”). Samoin luo data/README.md jossa selität data-hakemiston rakenteen (voit osin kopioida siitä, mitä edellä kirjoitettiin). Nämä README:t antavat pohjan projektille ja ovat ensimmäinen paikka, josta muut näkevät mitä tehdään.

3. **Muuttujasanakirjapohjat:** Kopioi Fear-of-Falling-projektista mahdollisesti olemassa oleva muuttujasanakirjapohja, tai luo tyhjä data/VARIABLE\_STANDARDIZATION.csv jossa laitat otsikkorivin valmiiksi (Source, Original\_Name, Standard\_Name, Description,...). Tee sama data/data\_dictionary.csv:lle (esim. sarakkeet: Variable, Label, Type, Units, Allowed, Source). Kirjoita Muuttujasanakirja.md:n alkuun lista keskeisistä muuttujista tiedossasi (FOF\_status, etc.) ja laadi niille lyhyet selitteet nyt heti. Vaikket vielä tiedä tarkkoja arvoja, voit laittaa placeholderit. Tallenna ja commitoi. Tämä vie 20-30 min, mutta antaa sinulle ja tiimille rungon, jota täyttää, kun varsinainen data saapuu.

4. **Environmentin pystytys:** Varmista että sinulla on Python/R ympäristö valmiina. Luo requirements.txt ja lisää sinne tiedossa olevat paketit (pandas, numpy, possibly langchain tms., R-puolelle renv::init() jos käytät R:ää). Aja ympäristö pystyyn (esim. pip install \-r requirements.txt). Tämä askeleella varmistat, että voit heti alkaa käsitellä dataa kun se tulee.

5. **Datan inventaario (jos tiedostolista saatavilla):** Jos organisaatiolta on jo saatu lista paper\_02-kansion tiedostoista (nimet), käytä sitä: luo manifest/dataset\_manifest.csv ja syötä tiedostonimet sinne. Voit nyt jo kirjoittaa käsin niiden kuvaukset ja arvioida sisällöt (esim. “MFFP\_cohort.csv – baseline survey data, 1010 rows”; “HILMO\_extract.xlsx – hospital visit records 2017-2019”). Lisää ainakin kolumnit Name ja Description; MD5 ja koko saat myöhemmin kun tiedostot käsillä. Commitoi tämä manifestiluonnos.

6. **Ensimmäinen testiajo pienellä datalla:** Luo scripts/preprocess\_data.py (tai .R) ja kirjoita sinne koodi, joka lukee esimerkiksi pienen testidatan (voit luoda itse pari riviä dummy-CSV:hen data/external/test.csv). Kokeile skriptiä: esim. laske dummydataan joku tunnusluku ja tulosta. Tämä on “end-to-end smoke test”, että koodiputki toimii edes pseudodatan läpi. Kun oikea data tulee, vaihdat vain polun. Nyt ajo todennäköisesti toimii heti, koska ympäristö on asetettu ja polkurakenne paikoillaan.

7. **QA-kehikon aloitus:** Kirjoita yksi yksinkertainen testifunktio tests/test\_schema.py:ssä, vaikkapa assertio että VARIABLE\_STANDARDIZATION.csv sisältää tietyt otsikot. Aja testi (pytest). Se todennäköisesti menee läpi. Tämä on minimaalisin testiesimerkki, mutta se osoittaa tiimille, että testirunko on olemassa. Myöhemmin lisäät oikeat testit.

8. **Agentti-indeksoinnin valmistelu:** Päätä alustavasti, miten aiot agentin toteuttaa. Esim. jos käytössä on Jupyter \+ LangChain, voit jo asentaa pip install langchain faiss-cpu. Luo tools/\-kansioon notebook tai python moduuli, jossa aloitat kokeilun: indeksöi Muuttujasanakirja.md teksti vektoriavaruuteen ja testaa yksinkertaisella kysymyksellä (“Mikä on FOF\_status?”), saatko haulla relevantin pätkän. Tämä varmistaa, että kun varsinainen sisältö on paikoillaan, agentti-integraatio sujuu.

Ensimmäisen 60 minuutin aikana edellä mainituilla askelilla saat aikaan toimivan rungon projektille. Sen jälkeen voit keskittyä syventymään oikean datan sisältöön ja täydentämään dokumentaation yksityiskohdat. Kiitos systemaattisen rakenteen, jokainen lisäminuutti menee hyödylliseen täyttötyöhön eikä rakenteen korjailuun. Projektisi on nyt hyvin pohjustettu tekoälyavusteista analyysiä varten – data on siistissä kunnossa, dokumentaatio kattavaa ja agentti pääsee käsiksi juuri siihen tietoon mitä se tarvitsee, ei mihinkään enempään. Tämä luo erinomaiset edellytykset luotettavalle ja tehokkaalle RAG-analyysille.

**Lähteet:** Kaikki yllä esitetyt suositukset ja periaatteet pohjautuvat tunnettuihin hyviin käytäntöihin ja dokumentoituihin lähteisiin: Cookiecutter Data Science \-malliin datan organisoinnissa (esim. *”Immutable raw data… ensures reproducibility”*[\[9\]](https://www.linkedin.com/posts/pablo-arino_so-true-for-data-sciencemachine-learning-activity-7338899099816972288-viQS#:~:text=So%20true%21%20For%20data%20science%2Fmachine,tool%20for%20handling%20both%20dependency)), Harvardin ja Collaten ohjeisiin metadatan ja muuttujasanakirjojen tärkeydestä (muuttujien määritelmät, yksi totuuden lähde)[\[12\]](https://datamanagement.hms.harvard.edu/collect-analyze/documentation-metadata/data-dictionary#:~:text=%2A%20Variable%20names%20%2A%20Human,Definition%20of%20the%20variable)[\[11\]](https://www.intellectyx.com/top-10-best-practices-for-effective-data-standardization-in-2025/#:~:text=Maintain%20a%20Centralised%20Data%20Dictionary), sekä julkisiin RAG-käytäntöihin kuten Cohere:n ja AWS:n esimerkkeihin chunkkauksesta ja hallusinaatioiden ehkäisystä[\[17\]](https://docs.cohere.com/page/agentic-rag-mixed-data#:~:text=,that%20similar%20elements%20are%20grouped)[\[22\]](https://www.itconvergence.com/blog/how-to-overcome-ai-hallucinations-using-retrieval-augmented-generation/#:~:text=Retrieval,in%20reliable%2C%20external%20data%20sources). Näitä periaatteita noudattamalla *Quantify FOF* \-projekti on linjassa modernin tutkimusdatan hallinnan vaatimusten kanssa ja valmiina hyödyntämään tekoälyagenttien kykyjä turvallisesti ja tehokkaasti.

---

[\[1\]](https://agentskillsindex.com/en/skills/Tupatuko2023/Python-R-Scripts#:~:text=Diff,mismatches%2C%20and%20output%20discipline%20risks) fof-preflight \- AgentSkillsIndex

[https://agentskillsindex.com/en/skills/Tupatuko2023/Python-R-Scripts](https://agentskillsindex.com/en/skills/Tupatuko2023/Python-R-Scripts)

[\[2\]](https://mcpmarket.com/tools/skills/fear-of-falling-qc-summarizer#:~:text=,with%20format%20verification) [\[3\]](https://mcpmarket.com/tools/skills/fear-of-falling-qc-summarizer#:~:text=summaries,level%20data%20to%20ensure%20compliance) fof-qc-summarizer: Fear-of-Falling Claude Code Skill

[https://mcpmarket.com/tools/skills/fear-of-falling-qc-summarizer](https://mcpmarket.com/tools/skills/fear-of-falling-qc-summarizer)

[\[4\]](https://learning.nceas.ucsb.edu/2023-01-arctic/metadata-best-practices-and-data-publishing.html#:~:text=In%20brief%2C%20some%20of%20the,best%20practices%20to%20follow%20are) [\[16\]](https://learning.nceas.ucsb.edu/2023-01-arctic/metadata-best-practices-and-data-publishing.html#:~:text=,descriptive%20file%20names%20without%20spaces) 14 Metadata Best Practices and Data Publishing | Fundamentals in Data Management for Qualitative and Quantitative Arctic Research

[https://learning.nceas.ucsb.edu/2023-01-arctic/metadata-best-practices-and-data-publishing.html](https://learning.nceas.ucsb.edu/2023-01-arctic/metadata-best-practices-and-data-publishing.html)

[\[5\]](https://www.linkedin.com/posts/pablo-arino_so-true-for-data-sciencemachine-learning-activity-7338899099816972288-viQS#:~:text=cluttering%20them,foundation%20for%20scalable%2C%20maintainable%2C%20and) [\[9\]](https://www.linkedin.com/posts/pablo-arino_so-true-for-data-sciencemachine-learning-activity-7338899099816972288-viQS#:~:text=So%20true%21%20For%20data%20science%2Fmachine,tool%20for%20handling%20both%20dependency) Why I love Cookiecutter Data Science template | Pablo Ariño Fernández posted on the topic | LinkedIn

[https://www.linkedin.com/posts/pablo-arino\_so-true-for-data-sciencemachine-learning-activity-7338899099816972288-viQS](https://www.linkedin.com/posts/pablo-arino_so-true-for-data-sciencemachine-learning-activity-7338899099816972288-viQS)

[\[6\]](https://nlsp.nasa.gov/Training%20Sheet%20-%20Data%20Submission%20Guidelines.pdf#:~:text=A%20file%20manifest%20helps%20others,for%20readability) [\[7\]](https://nlsp.nasa.gov/Training%20Sheet%20-%20Data%20Submission%20Guidelines.pdf#:~:text=computing%20languages%20or%20built,with%20files%20will%20go%20undetected) [\[8\]](https://nlsp.nasa.gov/Training%20Sheet%20-%20Data%20Submission%20Guidelines.pdf#:~:text=%E2%80%A2%20Specify%20details%20for%20each,%28Continued%20back%20page) [\[13\]](https://nlsp.nasa.gov/Training%20Sheet%20-%20Data%20Submission%20Guidelines.pdf#:~:text=%E2%80%A2%20Include%20MD5%20checksums%20in,with%20files%20will%20go%20undetected) [\[14\]](https://nlsp.nasa.gov/Training%20Sheet%20-%20Data%20Submission%20Guidelines.pdf#:~:text=%E2%80%A2%20Tabular%20data%20should%20be,redundant%20data%20entries%20are%20present) [\[15\]](https://nlsp.nasa.gov/Training%20Sheet%20-%20Data%20Submission%20Guidelines.pdf#:~:text=%E2%80%A2%20Ensure%20all%20values%20are,mm%2Fdd%2Fyyyy%20for%20dates) Data Submission Guidelines

[https://nlsp.nasa.gov/Training%20Sheet%20-%20Data%20Submission%20Guidelines.pdf](https://nlsp.nasa.gov/Training%20Sheet%20-%20Data%20Submission%20Guidelines.pdf)

[\[10\]](https://datamanagement.hms.harvard.edu/collect-analyze/documentation-metadata/data-dictionary#:~:text=A%20data%20dictionary%20is%20a,others%20to%20understand%20your%20data) [\[12\]](https://datamanagement.hms.harvard.edu/collect-analyze/documentation-metadata/data-dictionary#:~:text=%2A%20Variable%20names%20%2A%20Human,Definition%20of%20the%20variable) Data Dictionary | Data Management

[https://datamanagement.hms.harvard.edu/collect-analyze/documentation-metadata/data-dictionary](https://datamanagement.hms.harvard.edu/collect-analyze/documentation-metadata/data-dictionary)

[\[11\]](https://www.intellectyx.com/top-10-best-practices-for-effective-data-standardization-in-2025/#:~:text=Maintain%20a%20Centralised%20Data%20Dictionary) Top 10 Tips for Data Standardization Success in 2025

[https://www.intellectyx.com/top-10-best-practices-for-effective-data-standardization-in-2025/](https://www.intellectyx.com/top-10-best-practices-for-effective-data-standardization-in-2025/)

[\[17\]](https://docs.cohere.com/page/agentic-rag-mixed-data#:~:text=,that%20similar%20elements%20are%20grouped) [\[25\]](https://docs.cohere.com/page/agentic-rag-mixed-data#:~:text=More%20broadly%2C%20the%20implementation%20of,the%20model%20access%20to%20tools) Agentic RAG for PDFs with mixed data | Cohere

[https://docs.cohere.com/page/agentic-rag-mixed-data](https://docs.cohere.com/page/agentic-rag-mixed-data)

[\[18\]](https://docs.cohere.com/page/chunking-strategies#:~:text=,n%60%20for%20paragraphs) [\[19\]](https://docs.cohere.com/page/chunking-strategies#:~:text=%23%20Content) Effective Chunking Strategies for RAG | Cohere

[https://docs.cohere.com/page/chunking-strategies](https://docs.cohere.com/page/chunking-strategies)

[\[20\]](https://www.getcollate.io/learning-center/data-dictionary#:~:text=The%20primary%20purpose%20of%20a,where%20inconsistencies%20can%20disrupt%20operations) [\[21\]](https://www.getcollate.io/learning-center/data-dictionary#:~:text=A%20data%20dictionary%20supports%20collaboration,traceability%20of%20the%20data%20lifecycle) Data Dictionary in 2025 \- 5 Use Cases & 5 Critical Best Practices | Collate Learning Center

[https://www.getcollate.io/learning-center/data-dictionary](https://www.getcollate.io/learning-center/data-dictionary)

[\[22\]](https://www.itconvergence.com/blog/how-to-overcome-ai-hallucinations-using-retrieval-augmented-generation/#:~:text=Retrieval,in%20reliable%2C%20external%20data%20sources) [\[23\]](https://www.itconvergence.com/blog/how-to-overcome-ai-hallucinations-using-retrieval-augmented-generation/#:~:text=Use%20Structured%20Data%20for%20Retrieval) [\[24\]](https://www.itconvergence.com/blog/how-to-overcome-ai-hallucinations-using-retrieval-augmented-generation/#:~:text=...%20www.itconvergence.com%20%20Retrieval,in%20reliable%2C%20external%20data%20sources) How to Prevent AI Hallucinations with Retrieval Augmented Generation

[https://www.itconvergence.com/blog/how-to-overcome-ai-hallucinations-using-retrieval-augmented-generation/](https://www.itconvergence.com/blog/how-to-overcome-ai-hallucinations-using-retrieval-augmented-generation/)